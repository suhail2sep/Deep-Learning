{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "vsvCJo23s2U6"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step-2**"
      ],
      "metadata": {
        "id": "Mp8g8UDY9oRx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loading the Dataset**"
      ],
      "metadata": {
        "id": "2NQBgh3--JIB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "# Load QMNIST dataset and preprocess\n",
        "train_dataset = datasets.QMNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.QMNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
      ],
      "metadata": {
        "id": "La9jV5F50Ime"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Visualizing the data**"
      ],
      "metadata": {
        "id": "CqujdlFT-UNs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Numeric to string label mapping\n",
        "digit_labels = {\n",
        "    0: \"Zero\", 1: \"One\", 2: \"Two\", 3: \"Three\", 4: \"Four\",\n",
        "    5: \"Five\", 6: \"Six\", 7: \"Seven\", 8: \"Eight\", 9: \"Nine\"\n",
        "}\n",
        "\n",
        "# Image processing steps\n",
        "image_transforms = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))  # Added normalization\n",
        "])\n",
        "\n",
        "# Loading the QMNIST training data\n",
        "qmnist_training = datasets.QMNIST(root='./data_qmnist', train=True, download=True, transform=image_transforms)\n",
        "\n",
        "# Setting up the figure for visualization\n",
        "visual_figure = plt.figure(figsize=(10, 10))  # Adjusted figure size\n",
        "columns, rows = 3, 3  # Kept the same but can change as needed\n",
        "\n",
        "# Displaying images from the dataset\n",
        "for i in range(1, columns * rows + 1):\n",
        "    sample_index = torch.randint(len(qmnist_training), size=(1,)).item()\n",
        "    image, digit = qmnist_training[sample_index]\n",
        "    visual_figure.add_subplot(rows, columns, i)\n",
        "    plt.title(digit_labels[digit])\n",
        "    plt.axis(\"off\")\n",
        "    plt.imshow(image.squeeze(), cmap=\"gray\")  # Ensuring grayscale display\n",
        "\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 829
        },
        "id": "a2st0sLXwEPr",
        "outputId": "7855fbb8-df11-41fd-d968-3a910bcc8b81"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x1000 with 9 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Defining the neural network**"
      ],
      "metadata": {
        "id": "UiIIjS_F-w8M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "# Define a custom neural network architecture\n",
        "class CustomFeedforwardNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CustomFeedforwardNN, self).__init__()\n",
        "        # Define the first fully connected layer\n",
        "        self.first_layer = nn.Linear(784, 128)  # 28*28 pixels input, 128 neurons\n",
        "        # Define the second fully connected layer\n",
        "        self.second_layer = nn.Linear(128, 64)  # 128 neurons, 64 neurons\n",
        "        # Define the output layer\n",
        "        self.output_layer = nn.Linear(64, 10)  # 64 neurons, 10 output classes\n",
        "\n",
        "    def forward(self, input_tensor):\n",
        "        # Flatten the input tensor to fit the input layer\n",
        "        input_tensor = input_tensor.view(-1, 784)  # Flatten the image to a vector\n",
        "        # Apply ReLU activation function after first layer\n",
        "        input_tensor = torch.relu(self.first_layer(input_tensor))\n",
        "        # Apply ReLU activation function after second layer\n",
        "        input_tensor = torch.relu(self.second_layer(input_tensor))\n",
        "        # No activation function after the output layer\n",
        "        input_tensor = self.output_layer(input_tensor)\n",
        "        return input_tensor\n",
        "\n",
        "# Initialize the custom neural network\n",
        "custom_model = CustomFeedforwardNN()\n"
      ],
      "metadata": {
        "id": "SRTjszvG0Pe5"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# Configure the loss function and optimizer for the custom model\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "model_optimizer = optim.Adam(custom_model.parameters(), lr=0.001)  # Adjusted learning rate if needed\n",
        "\n",
        "# Set the number of training epochs\n",
        "training_epochs = 10\n",
        "\n",
        "# Start the training process\n",
        "for epoch in range(training_epochs):\n",
        "    custom_model.train()  # Ensure the model is in training mode\n",
        "    total_loss = 0.0  # Accumulate loss for reporting\n",
        "\n",
        "    # Iterate over the training data\n",
        "    for batch_index, (inputs, targets) in enumerate(train_loader):\n",
        "        model_optimizer.zero_grad()  # Clear gradients for the next train steps\n",
        "        predictions = custom_model(inputs)  # Generate predictions\n",
        "        batch_loss = loss_function(predictions, targets)  # Calculate loss\n",
        "        batch_loss.backward()  # Backpropagate the error\n",
        "        model_optimizer.step()  # Adjust model parameters\n",
        "\n",
        "        total_loss += batch_loss.item()  # Update total loss\n",
        "        # Log the loss every 200 mini-batches instead of 100 for a change\n",
        "        if batch_index % 200 == 199:\n",
        "            print(f'Epoch {epoch + 1}, Batch {batch_index + 1}, Loss: {total_loss / 200:.4f}')\n",
        "            total_loss = 0.0  # Reset total loss after logging\n",
        "\n",
        "print('Training Complete')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n-HkJ_9i0-J-",
        "outputId": "f2fe710e-0f76-456b-cd72-08b97b2141c9"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Batch 200, Loss: 0.7204\n",
            "Epoch 1, Batch 400, Loss: 0.3526\n",
            "Epoch 1, Batch 600, Loss: 0.3063\n",
            "Epoch 1, Batch 800, Loss: 0.2600\n",
            "Epoch 2, Batch 200, Loss: 0.2190\n",
            "Epoch 2, Batch 400, Loss: 0.1895\n",
            "Epoch 2, Batch 600, Loss: 0.1782\n",
            "Epoch 2, Batch 800, Loss: 0.1641\n",
            "Epoch 3, Batch 200, Loss: 0.1385\n",
            "Epoch 3, Batch 400, Loss: 0.1364\n",
            "Epoch 3, Batch 600, Loss: 0.1361\n",
            "Epoch 3, Batch 800, Loss: 0.1218\n",
            "Epoch 4, Batch 200, Loss: 0.1135\n",
            "Epoch 4, Batch 400, Loss: 0.1095\n",
            "Epoch 4, Batch 600, Loss: 0.1073\n",
            "Epoch 4, Batch 800, Loss: 0.1015\n",
            "Epoch 5, Batch 200, Loss: 0.0894\n",
            "Epoch 5, Batch 400, Loss: 0.0823\n",
            "Epoch 5, Batch 600, Loss: 0.0894\n",
            "Epoch 5, Batch 800, Loss: 0.0959\n",
            "Epoch 6, Batch 200, Loss: 0.0753\n",
            "Epoch 6, Batch 400, Loss: 0.0704\n",
            "Epoch 6, Batch 600, Loss: 0.0758\n",
            "Epoch 6, Batch 800, Loss: 0.0768\n",
            "Epoch 7, Batch 200, Loss: 0.0668\n",
            "Epoch 7, Batch 400, Loss: 0.0734\n",
            "Epoch 7, Batch 600, Loss: 0.0690\n",
            "Epoch 7, Batch 800, Loss: 0.0675\n",
            "Epoch 8, Batch 200, Loss: 0.0596\n",
            "Epoch 8, Batch 400, Loss: 0.0577\n",
            "Epoch 8, Batch 600, Loss: 0.0681\n",
            "Epoch 8, Batch 800, Loss: 0.0673\n",
            "Epoch 9, Batch 200, Loss: 0.0506\n",
            "Epoch 9, Batch 400, Loss: 0.0664\n",
            "Epoch 9, Batch 600, Loss: 0.0448\n",
            "Epoch 9, Batch 800, Loss: 0.0592\n",
            "Epoch 10, Batch 200, Loss: 0.0503\n",
            "Epoch 10, Batch 400, Loss: 0.0505\n",
            "Epoch 10, Batch 600, Loss: 0.0504\n",
            "Epoch 10, Batch 800, Loss: 0.0548\n",
            "Training Complete\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step-3**"
      ],
      "metadata": {
        "id": "UTg22Kdx-_J2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluating the results**"
      ],
      "metadata": {
        "id": "OpnIXCHP_KfG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the model to evaluation mode\n",
        "custom_model.eval()\n",
        "\n",
        "# Initialize counters for correct predictions and the total number of samples\n",
        "correct_predictions = 0\n",
        "total_samples = 0\n",
        "\n",
        "# Disable gradient calculation for evaluation, which saves memory and computations\n",
        "with torch.no_grad():\n",
        "    for batch in train_loader:  # Iterate over the entire dataset\n",
        "        batch_images, batch_labels = batch  # Unpack the batch\n",
        "        batch_outputs = custom_model(batch_images)  # Get model predictions\n",
        "        _, predicted_labels = torch.max(batch_outputs.data, 1)  # Find the predicted class\n",
        "\n",
        "        total_samples += batch_labels.size(0)  # Update total sample count\n",
        "        correct_predictions += (predicted_labels == batch_labels).sum().item()  # Count correct predictions\n",
        "\n",
        "# Calculate and print the accuracy\n",
        "accuracy = correct_predictions / total_samples * 100\n",
        "print(f'Accuracy on the training set: {accuracy:.3f}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j4sQvbON2C51",
        "outputId": "b6556845-5b97-4235-b715-4a1df6c8d9dd"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on the training set: 98.615%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Switch the model to evaluation mode to disable dropout and batch normalization effects\n",
        "custom_model.eval()\n",
        "\n",
        "# Initialize counters for accurately predicted samples and the total count of samples\n",
        "accurate_predictions = 0\n",
        "samples_count = 0\n",
        "\n",
        "# Deactivate gradient tracking for evaluation to reduce memory consumption and computations\n",
        "with torch.no_grad():\n",
        "    for batch_data in test_loader:  # Iterate through each batch in the test dataset\n",
        "        test_images, test_labels = batch_data  # Unpack images and labels from the current batch\n",
        "        test_outputs = custom_model(test_images)  # Compute the model's predictions\n",
        "\n",
        "        # Determine the predicted class with the highest score for each image\n",
        "        _, predictions = torch.max(test_outputs.data, 1)\n",
        "\n",
        "        # Update the total number of samples processed\n",
        "        samples_count += test_labels.size(0)\n",
        "\n",
        "        # Increment the count of correct predictions\n",
        "        accurate_predictions += (predictions == test_labels).sum().item()\n",
        "\n",
        "# Calculate the overall accuracy by dividing the number of correct predictions by the total number of samples\n",
        "accuracy_percentage = (accurate_predictions / samples_count) * 100\n",
        "\n",
        "# Display the calculated accuracy on the test set\n",
        "print(f'Accuracy on the test set: {accuracy_percentage:.3f}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Sl89YFD2YXJ",
        "outputId": "240bd43b-ef4d-4936-e05f-262109b6cf4f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on the test set: 97.170%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming 'model_predictions' contains predicted labels from the previous code snippet\n",
        "# and 'test_loader' was used to obtain 'images' and 'labels'\n",
        "\n",
        "# Select the first test image and its corresponding label for demonstration\n",
        "test_image, actual_label = images[0], labels[0]\n",
        "\n",
        "# Reshape the test image to its original 2D shape (28x28 for MNIST)\n",
        "reshaped_test_image = test_image.view(28, 28)\n",
        "\n",
        "# Convert the reshaped test image to a numpy array for plotting\n",
        "image_for_plot = reshaped_test_image.numpy()\n",
        "\n",
        "# Display the test image\n",
        "plt.imshow(image_for_plot, cmap='gray')  # Use grayscale color map for visualization\n",
        "# Title with predicted and actual label. Assuming the first prediction corresponds to the first test image\n",
        "plt.title(f'Predicted Label: {model_predictions[0]}, Actual Label: {actual_label.item()}')\n",
        "plt.axis('off')  # Hide axes for better visual presentation\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "id": "q4PJxJjY2wBe",
        "outputId": "24820373-3248-48a6-c637-5784a32e0d4f"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Accuracy of training set=98.61**\n",
        "**Accuracy of testing data=97.17**"
      ],
      "metadata": {
        "id": "KmmAkpwm_Y4E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step-4**\n"
      ],
      "metadata": {
        "id": "xI80Oo8Z369D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Increase the current number of nodes in the layer to 256**"
      ],
      "metadata": {
        "id": "AovCNSY4_1l5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hypothesis-\n",
        "It is expected that increasing  the numbert of nodes within the model would lead to improved outcomes, as this increase in nodes enhances the model's capability to encapsulate complex interactions among the input variables, thereby strengthening its representational power."
      ],
      "metadata": {
        "id": "ZxvGpwxaAB3R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step-5**"
      ],
      "metadata": {
        "id": "ijuQJA3xA39q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Define an enhanced neural network architecture\n",
        "class EnhancedMLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(EnhancedMLP, self).__init__()\n",
        "        # Adjusting layer sizes as per proposed modification\n",
        "        self.dense1 = nn.Linear(784, 256)  # From input images to hidden layer 1\n",
        "        self.dense2 = nn.Linear(256, 256)  # Hidden layer 1 to hidden layer 2\n",
        "        self.dense3 = nn.Linear(256, 10)   # Hidden layer 2 to output layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Flatten the input and pass through the network\n",
        "        x = x.view(-1, 784)  # Flatten image to 1D tensor\n",
        "        x = torch.relu(self.dense1(x))\n",
        "        x = torch.relu(self.dense2(x))\n",
        "        x = self.dense3(x)\n",
        "        return x\n",
        "\n",
        "# Initialize the enhanced model\n",
        "enhanced_model = EnhancedMLP()\n",
        "\n",
        "# Setup the loss function and optimizer\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(enhanced_model.parameters(), lr=0.001)\n",
        "\n",
        "# Training the enhanced neural network\n",
        "epochs = 10\n",
        "for epoch in range(epochs):\n",
        "    enhanced_model.train()\n",
        "    epoch_loss = 0.0\n",
        "    for i, (inputs, targets) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        outputs = enhanced_model(inputs)\n",
        "        loss = loss_function(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "        if i % 100 == 99:\n",
        "            print(f'Epoch {epoch + 1}, Step {i + 1}: Avg Loss = {epoch_loss / 100:.4f}')\n",
        "            epoch_loss = 0.0\n",
        "\n",
        "print('Training Complete')\n",
        "\n",
        "# Evaluating the model's performance on the training set\n",
        "enhanced_model.eval()\n",
        "train_correct, train_total = 0, 0\n",
        "with torch.no_grad():\n",
        "    for inputs, targets in train_loader:\n",
        "        outputs = enhanced_model(inputs)\n",
        "        _, predictions = torch.max(outputs, 1)\n",
        "        train_total += targets.size(0)\n",
        "        train_correct += (predictions == targets).sum().item()\n",
        "print(f'\\nTraining Set Accuracy: {train_correct / train_total * 100:.2f}%')\n",
        "\n",
        "# Evaluating the model's performance on the test set\n",
        "test_correct, test_total = 0, 0\n",
        "with torch.no_grad():\n",
        "    for inputs, targets in test_loader:\n",
        "        outputs = enhanced_model(inputs)\n",
        "        _, predictions = torch.max(outputs, 1)\n",
        "        test_total += targets.size(0)\n",
        "        test_correct += (predictions == targets).sum().item()\n",
        "print(f'\\nTest Set Accuracy: {test_correct / test_total * 100:.2f}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TmKY2Flk37VY",
        "outputId": "80440c5d-be0c-4e46-fa86-ba1bf50fb2de"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Step 100: Avg Loss = 0.8546\n",
            "Epoch 1, Step 200: Avg Loss = 0.3999\n",
            "Epoch 1, Step 300: Avg Loss = 0.3522\n",
            "Epoch 1, Step 400: Avg Loss = 0.2904\n",
            "Epoch 1, Step 500: Avg Loss = 0.2483\n",
            "Epoch 1, Step 600: Avg Loss = 0.2401\n",
            "Epoch 1, Step 700: Avg Loss = 0.1913\n",
            "Epoch 1, Step 800: Avg Loss = 0.1940\n",
            "Epoch 1, Step 900: Avg Loss = 0.1963\n",
            "Epoch 2, Step 100: Avg Loss = 0.1636\n",
            "Epoch 2, Step 200: Avg Loss = 0.1513\n",
            "Epoch 2, Step 300: Avg Loss = 0.1379\n",
            "Epoch 2, Step 400: Avg Loss = 0.1615\n",
            "Epoch 2, Step 500: Avg Loss = 0.1563\n",
            "Epoch 2, Step 600: Avg Loss = 0.1292\n",
            "Epoch 2, Step 700: Avg Loss = 0.1383\n",
            "Epoch 2, Step 800: Avg Loss = 0.1369\n",
            "Epoch 2, Step 900: Avg Loss = 0.1257\n",
            "Epoch 3, Step 100: Avg Loss = 0.1124\n",
            "Epoch 3, Step 200: Avg Loss = 0.1148\n",
            "Epoch 3, Step 300: Avg Loss = 0.1098\n",
            "Epoch 3, Step 400: Avg Loss = 0.1108\n",
            "Epoch 3, Step 500: Avg Loss = 0.1045\n",
            "Epoch 3, Step 600: Avg Loss = 0.1130\n",
            "Epoch 3, Step 700: Avg Loss = 0.0917\n",
            "Epoch 3, Step 800: Avg Loss = 0.1015\n",
            "Epoch 3, Step 900: Avg Loss = 0.1022\n",
            "Epoch 4, Step 100: Avg Loss = 0.0737\n",
            "Epoch 4, Step 200: Avg Loss = 0.0954\n",
            "Epoch 4, Step 300: Avg Loss = 0.0766\n",
            "Epoch 4, Step 400: Avg Loss = 0.0723\n",
            "Epoch 4, Step 500: Avg Loss = 0.0930\n",
            "Epoch 4, Step 600: Avg Loss = 0.0850\n",
            "Epoch 4, Step 700: Avg Loss = 0.0850\n",
            "Epoch 4, Step 800: Avg Loss = 0.0884\n",
            "Epoch 4, Step 900: Avg Loss = 0.0894\n",
            "Epoch 5, Step 100: Avg Loss = 0.0696\n",
            "Epoch 5, Step 200: Avg Loss = 0.0850\n",
            "Epoch 5, Step 300: Avg Loss = 0.0607\n",
            "Epoch 5, Step 400: Avg Loss = 0.0822\n",
            "Epoch 5, Step 500: Avg Loss = 0.0802\n",
            "Epoch 5, Step 600: Avg Loss = 0.0713\n",
            "Epoch 5, Step 700: Avg Loss = 0.0779\n",
            "Epoch 5, Step 800: Avg Loss = 0.0789\n",
            "Epoch 5, Step 900: Avg Loss = 0.0755\n",
            "Epoch 6, Step 100: Avg Loss = 0.0648\n",
            "Epoch 6, Step 200: Avg Loss = 0.0495\n",
            "Epoch 6, Step 300: Avg Loss = 0.0698\n",
            "Epoch 6, Step 400: Avg Loss = 0.0544\n",
            "Epoch 6, Step 500: Avg Loss = 0.0544\n",
            "Epoch 6, Step 600: Avg Loss = 0.0745\n",
            "Epoch 6, Step 700: Avg Loss = 0.0748\n",
            "Epoch 6, Step 800: Avg Loss = 0.0685\n",
            "Epoch 6, Step 900: Avg Loss = 0.0688\n",
            "Epoch 7, Step 100: Avg Loss = 0.0495\n",
            "Epoch 7, Step 200: Avg Loss = 0.0480\n",
            "Epoch 7, Step 300: Avg Loss = 0.0469\n",
            "Epoch 7, Step 400: Avg Loss = 0.0639\n",
            "Epoch 7, Step 500: Avg Loss = 0.0646\n",
            "Epoch 7, Step 600: Avg Loss = 0.0543\n",
            "Epoch 7, Step 700: Avg Loss = 0.0572\n",
            "Epoch 7, Step 800: Avg Loss = 0.0678\n",
            "Epoch 7, Step 900: Avg Loss = 0.0643\n",
            "Epoch 8, Step 100: Avg Loss = 0.0502\n",
            "Epoch 8, Step 200: Avg Loss = 0.0493\n",
            "Epoch 8, Step 300: Avg Loss = 0.0436\n",
            "Epoch 8, Step 400: Avg Loss = 0.0495\n",
            "Epoch 8, Step 500: Avg Loss = 0.0450\n",
            "Epoch 8, Step 600: Avg Loss = 0.0501\n",
            "Epoch 8, Step 700: Avg Loss = 0.0596\n",
            "Epoch 8, Step 800: Avg Loss = 0.0527\n",
            "Epoch 8, Step 900: Avg Loss = 0.0525\n",
            "Epoch 9, Step 100: Avg Loss = 0.0305\n",
            "Epoch 9, Step 200: Avg Loss = 0.0377\n",
            "Epoch 9, Step 300: Avg Loss = 0.0329\n",
            "Epoch 9, Step 400: Avg Loss = 0.0472\n",
            "Epoch 9, Step 500: Avg Loss = 0.0504\n",
            "Epoch 9, Step 600: Avg Loss = 0.0518\n",
            "Epoch 9, Step 700: Avg Loss = 0.0472\n",
            "Epoch 9, Step 800: Avg Loss = 0.0504\n",
            "Epoch 9, Step 900: Avg Loss = 0.0539\n",
            "Epoch 10, Step 100: Avg Loss = 0.0312\n",
            "Epoch 10, Step 200: Avg Loss = 0.0389\n",
            "Epoch 10, Step 300: Avg Loss = 0.0408\n",
            "Epoch 10, Step 400: Avg Loss = 0.0305\n",
            "Epoch 10, Step 500: Avg Loss = 0.0489\n",
            "Epoch 10, Step 600: Avg Loss = 0.0543\n",
            "Epoch 10, Step 700: Avg Loss = 0.0380\n",
            "Epoch 10, Step 800: Avg Loss = 0.0515\n",
            "Epoch 10, Step 900: Avg Loss = 0.0517\n",
            "Training Complete\n",
            "\n",
            "Training Set Accuracy: 99.01%\n",
            "\n",
            "Test Set Accuracy: 97.31%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step-6**"
      ],
      "metadata": {
        "id": "aQNpzychBFyD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training Set Accuracy: 99.01%\n",
        "\n",
        "Test Set Accuracy: 97.31%\n",
        "\n",
        "As we can see,increasing the number of nodes increases the accuracy slightly\n"
      ],
      "metadata": {
        "id": "wem5ZNFkBMNk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step-7**"
      ],
      "metadata": {
        "id": "bnzDiK3fB8zg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Experiment with different optimizers, loss functions, dropout, and activation functions, and observe the change in performance as you tune these hyperparameters."
      ],
      "metadata": {
        "id": "OT4xWzt-CBvo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Experiment-1**\n",
        "Using SGD optimizer instead of Adam"
      ],
      "metadata": {
        "id": "buA7I6LqCILT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Define the neural network architecture with the original configuration\n",
        "class EnhancedMLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(EnhancedMLP, self).__init__()\n",
        "        self.dense1 = nn.Linear(784, 256)\n",
        "        self.dense2 = nn.Linear(256, 256)\n",
        "        self.dense3 = nn.Linear(256, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 784)\n",
        "        x = torch.relu(self.dense1(x))\n",
        "        x = torch.relu(self.dense2(x))\n",
        "        x = self.dense3(x)\n",
        "        return x\n",
        "\n",
        "# Initialize the model\n",
        "model_exp1 = EnhancedMLP()\n",
        "\n",
        "# Setup the loss function\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "\n",
        "# Use SGD optimizer instead of Adam\n",
        "optimizer = optim.SGD(model_exp1.parameters(), lr=0.01, momentum=0.9)\n",
        "\n",
        "# Placeholder for train_loader and test_loader\n",
        "# Assuming train_loader and test_loader are predefined\n",
        "\n",
        "# Training loop\n",
        "epochs = 10\n",
        "for epoch in range(epochs):\n",
        "    model_exp1.train()\n",
        "    for inputs, targets in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model_exp1(inputs)\n",
        "        loss = loss_function(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f'Epoch {epoch + 1} complete.')\n",
        "\n",
        "# Evaluation on training set\n",
        "correct, total = 0, 0\n",
        "model_exp1.eval()\n",
        "with torch.no_grad():\n",
        "    for inputs, targets in train_loader:\n",
        "        outputs = model_exp1(inputs)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += targets.size(0)\n",
        "        correct += (predicted == targets).sum().item()\n",
        "print(f'\\nTraining Set Accuracy: {100 * correct / total}%')\n",
        "\n",
        "# Evaluation on test set\n",
        "correct, total = 0, 0\n",
        "with torch.no_grad():\n",
        "    for inputs, targets in test_loader:\n",
        "        outputs = model_exp1(inputs)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += targets.size(0)\n",
        "        correct += (predicted == targets).sum().item()\n",
        "print(f'Test Set Accuracy: {100 * correct / total}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l5vpEkyb68Hz",
        "outputId": "4749732a-efed-4610-d264-4a5e747f2df5"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 complete.\n",
            "Epoch 2 complete.\n",
            "Epoch 3 complete.\n",
            "Epoch 4 complete.\n",
            "Epoch 5 complete.\n",
            "Epoch 6 complete.\n",
            "Epoch 7 complete.\n",
            "Epoch 8 complete.\n",
            "Epoch 9 complete.\n",
            "Epoch 10 complete.\n",
            "\n",
            "Training Set Accuracy: 99.155%\n",
            "Test Set Accuracy: 97.50166666666667%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observation: There is a slight change in model performance.**"
      ],
      "metadata": {
        "id": "klrkshEVDL9U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Experiment-2**\n",
        "Introducing Dropout and Using LeakyReLU Activation Function"
      ],
      "metadata": {
        "id": "5hng2SiyCeOD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define the enhanced neural network architecture with dropout and LeakyReLU\n",
        "class EnhancedMLPWithDropoutAndLeakyReLU(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(EnhancedMLPWithDropoutAndLeakyReLU, self).__init__()\n",
        "        self.dense1 = nn.Linear(784, 256)\n",
        "        self.dropout1 = nn.Dropout(0.5)  # Add dropout with a probability of 0.5\n",
        "        self.dense2 = nn.Linear(256, 256)\n",
        "        self.dropout2 = nn.Dropout(0.5)  # Add another dropout layer\n",
        "        self.dense3 = nn.Linear(256, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 784)\n",
        "        x = torch.nn.functional.leaky_relu(self.dense1(x))  # Use LeakyReLU\n",
        "        x = self.dropout1(x)\n",
        "        x = torch.nn.functional.leaky_relu(self.dense2(x))  # Use LeakyReLU again\n",
        "        x = self.dropout2(x)\n",
        "        x = self.dense3(x)\n",
        "        return x\n",
        "\n",
        "# Initialize the model with dropout and LeakyReLU\n",
        "model_exp2 = EnhancedMLPWithDropoutAndLeakyReLU()\n",
        "\n",
        "# Setup the loss function\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "\n",
        "# Use Adam optimizer (or any other optimizer of choice)\n",
        "optimizer = optim.Adam(model_exp2.parameters(), lr=0.001)\n",
        "\n",
        "# Placeholder for train_loader and test_loader\n",
        "\n",
        "# Training loop\n",
        "epochs = 10\n",
        "for epoch in range(epochs):\n",
        "    model_exp2.train()\n",
        "    for inputs, targets in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model_exp2(inputs)\n",
        "        loss = loss_function(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f'Epoch {epoch + 1} complete.')\n",
        "\n",
        "# Evaluation on training set\n",
        "correct, total = 0, 0\n",
        "model_exp2.eval()\n",
        "with torch.no_grad():\n",
        "    for inputs, targets in train_loader:\n",
        "        outputs = model_exp2(inputs)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += targets.size(0)\n",
        "        correct += (predicted == targets).sum().item()\n",
        "print(f'\\nTraining Set Accuracy: {100 * correct / total}%')\n",
        "\n",
        "# Evaluation on test set\n",
        "correct, total = 0, 0\n",
        "with torch.no_grad():\n",
        "    for inputs, targets in test_loader:\n",
        "        outputs = model_exp2(inputs)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += targets.size(0)\n",
        "        correct += (predicted == targets).sum().item()\n",
        "print(f'Test Set Accuracy: {100 * correct / total}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hOjsmxrC7481",
        "outputId": "309f0a42-d922-4c56-b0b5-e1e6d3471e4d"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 complete.\n",
            "Epoch 2 complete.\n",
            "Epoch 3 complete.\n",
            "Epoch 4 complete.\n",
            "Epoch 5 complete.\n",
            "Epoch 6 complete.\n",
            "Epoch 7 complete.\n",
            "Epoch 8 complete.\n",
            "Epoch 9 complete.\n",
            "Epoch 10 complete.\n",
            "\n",
            "Training Set Accuracy: 97.375%\n",
            "Test Set Accuracy: 96.58%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observation: There is a decrease in model performance.**"
      ],
      "metadata": {
        "id": "PLxSFb6GDW6M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Experiment-3**\n",
        "Adding Batch Normalization and Changing to the RMSprop Optimizer\n"
      ],
      "metadata": {
        "id": "lJq0zrpMCzHU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "class EnhancedMLPWithBatchNorm(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(EnhancedMLPWithBatchNorm, self).__init__()\n",
        "        self.dense1 = nn.Linear(784, 256)\n",
        "        self.bn1 = nn.BatchNorm1d(256)  # Batch Normalization layer for first dense layer\n",
        "        self.dense2 = nn.Linear(256, 256)\n",
        "        self.bn2 = nn.BatchNorm1d(256)  # Batch Normalization layer for second dense layer\n",
        "        self.dense3 = nn.Linear(256, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 784)\n",
        "        x = self.bn1(self.dense1(x))\n",
        "        x = torch.relu(x)\n",
        "        x = self.bn2(self.dense2(x))\n",
        "        x = torch.relu(x)\n",
        "        x = self.dense3(x)\n",
        "        return x\n",
        "\n",
        "# Initialize the model with batch normalization\n",
        "model_exp3 = EnhancedMLPWithBatchNorm()\n",
        "\n",
        "# Setup the loss function\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "\n",
        "# Use RMSprop optimizer\n",
        "optimizer = optim.RMSprop(model_exp3.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "epochs = 10\n",
        "for epoch in range(epochs):\n",
        "    model_exp3.train()\n",
        "    for inputs, targets in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model_exp3(inputs)\n",
        "        loss = loss_function(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f'Epoch {epoch + 1} complete.')\n",
        "\n",
        "# Evaluation on training set\n",
        "correct, total = 0, 0\n",
        "model_exp3.eval()\n",
        "with torch.no_grad():\n",
        "    for inputs, targets in train_loader:\n",
        "        outputs = model_exp3(inputs)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += targets.size(0)\n",
        "        correct += (predicted == targets).sum().item()\n",
        "print(f'\\nTraining Set Accuracy: {100 * correct / total}%')\n",
        "\n",
        "# Evaluation on test set\n",
        "correct, total = 0, 0\n",
        "with torch.no_grad():\n",
        "    for inputs, targets in test_loader:\n",
        "        outputs = model_exp3(inputs)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += targets.size(0)\n",
        "        correct += (predicted == targets).sum().item()\n",
        "print(f'Test Set Accuracy: {100 * correct / total}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "di42rrMy9YLx",
        "outputId": "73422e84-4ed8-463f-fa40-8ba27621192b"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 complete.\n",
            "Epoch 2 complete.\n",
            "Epoch 3 complete.\n",
            "Epoch 4 complete.\n",
            "Epoch 5 complete.\n",
            "Epoch 6 complete.\n",
            "Epoch 7 complete.\n",
            "Epoch 8 complete.\n",
            "Epoch 9 complete.\n",
            "Epoch 10 complete.\n",
            "\n",
            "Training Set Accuracy: 99.725%\n",
            "Test Set Accuracy: 97.95166666666667%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observation:Model performance increases.**"
      ],
      "metadata": {
        "id": "7P1C1wO0DiwC"
      }
    }
  ]
}